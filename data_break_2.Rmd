---
title: "Data Break 2"
output:
      html_document:
        keep_md: true
---

```{r}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(BiostatsUHNplus)
library(lubridate)
library(stringr)
library(seewave)
library(signal)
library(gsignal)
library(tidymodels)
library(parsnip)
library(zoo)
library(ranger)
```

### Read in data

```{r}
data <- read_csv("data_clean.csv")
glimpse(data)

### Sort by id and time
data <- arrange(data, id, time_s)
data$id <- as.factor(data$id)

## Filter out non study activity
data <- dplyr::filter(data, activity != 99)

### Converting appropriate variables to factor
data <- data %>% mutate_at(c(2, 5, 6), factor)

### Creating a second level variable

data$time <- gsub("\\..*","", data$time_s)
```

### Activity class

* 1. Walking
* 2. Descending stairs
* 3. Ascending stairs
* 4. Driving
* 77. Clapping 
* 99. Non-study activity

```{r}
table(data$activity)
```

### Feature development

Here are going to create some features to be included as predictors. These are just a few examples of what you might do. You would typically have more features and variation in your features including both time domain (shown here) and frequency domain (not shown... takes too long to run). 

```{r, warning = FALSE}
### Vector Magnitude
data$vec_mag <- sqrt((data$lw_x^2) + (data$lw_y^2) + (data$lw_z^2))

### 5 second Moving average of x, y, z

data$roll_mean_x <- rollmean(data$lw_x, 500, fill = mean(data$lw_x))
data$roll_mean_y <- rollmean(data$lw_y, 500, fill = mean(data$lw_y))
data$roll_mean_z <- rollmean(data$lw_z, 500, fill = mean(data$lw_z))

### Correlations between x, y, z

data_xyz <- select(data, lw_x, lw_y, lw_z)

### Correlation x y 
cor(data$lw_x, data$lw_y)
data$corr_x_y <- rollapply(data_xyz, width=500, function(x) cor(x[, 1], x[, 2]), by.column=FALSE, fill = -0.1848453)

### Correlation y z 
cor(data$lw_y, data$lw_z)
data$corr_y_z <- rollapply(data_xyz, width=500, function(x) cor(x[, 2], x[, 3]), by.column=FALSE, fill = -0.2885716)

### Correlation x z 
cor(data$lw_x, data$lw_z)
data$corr_x_z <- rollapply(data_xyz, width=500, function(x) cor(x[, 1], x[, 3]), by.column=FALSE, fill = 0.1069972)

#write_csv(data, "data_rf.csv")

rm(data_xyz)
#rm(data)
```

### Random Forest 

Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that works by creating a multitude of decision trees during training. For classification tasks, the output of the random forest is the class selected by most trees. It is one of most commonly used methods for physical activity classification... though a bit old now. [https://en.wikipedia.org/wiki/Random_forest](https://en.wikipedia.org/wiki/Random_forest)

We will use the Tidymodels framework for model fitting. Good tutorials for Tidymodels are below

* [https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/](https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/)
* [https://lsinks.github.io/posts/2023-04-10-tidymodels/tidymodels_tutorial.html](https://lsinks.github.io/posts/2023-04-10-tidymodels/tidymodels_tutorial.html)

We will use of a 5 fold cross validation and leave 2 test sets. Cross validation works as per the image below. In this example we will not finalize the analysis and run on the test set, just for the sake of time. 

![](https://static.wixstatic.com/media/ea0077_8bf9cf19b5ce4f24816ac8d7a1da00fd~mv2.png/v1/fill/w_804,h_452,al_c,q_90,usm_0.66_1.00_0.01,enc_auto/Resampling_PNG.png)

```{r}
#data <- read_csv("data_rf.csv")
#data$activity <- as.factor(data$activity)

# Fix the random numbers by setting the seed 
# This enables the analysis to be reproducible when random numbers are used 
set.seed(10)

data <- select(data, id, time_s, gender, age, weight_lbs, activity, lw_x, lw_y, lw_z, vec_mag, roll_mean_x, roll_mean_y, roll_mean_z, corr_x_y, corr_y_z, corr_x_z)

#### Cross Validation Split
cv_split <- initial_validation_split(data, 
                            strata = activity, 
                            prop = c(0.7, 0.2))

# Create data frames for the two sets:
train_data <- training(cv_split)
table(train_data$activity)

test_data  <- testing(cv_split)
table(test_data$activity)
```

High risk of data leakage with this method.

### Model 

Here we use the tidy models to setup a model using `ranger` and `classification` and we call the specific model we want to fit. `ranger` is the default package but there are additional engines you could use in [tidymodels](https://parsnip.tidymodels.org/reference/rand_forest.html)

* ranger
* aorsf
* h2o
* partykit
* randomForest
* spark

I'm fixing mtry = 5, min_n = 10, and tress = 10 just to make the model run more efficiently in class. These are hyperparameters for your model. 

* __mtry__: An integer for the number of predictors that will be randomly sampled at each split when creating the tree models.
* __trees__: An integer for the number of trees contained in the ensemble.
* __min_n__: An integer for the minimum number of data points in a node that are required for the node to be split further.

Normally you want to tune these to find the optimal values. This optimization process can take a long time and is best done using parallel processing. More information about model tuning using tidymodels in [this tutorial](https://juliasilge.com/blog/sf-trees-random-tuning/).

```{r}
### Set the number of cores on your computer
cores <- parallel::detectCores()
cores
```

Save the random forest model object. We set `num.threads = cores` because `ranger` will do parrallel processing so the modes will run more quickly. 

```{r}
rf_model <- rand_forest(mtry = 5, min_n = 10, trees = 10) %>% 
              set_engine("ranger", num.threads = cores) %>% 
              set_mode("classification")
```

#### Recipe

The recipe() function as we used it here has two arguments

1. A formula. Any variable on the left-hand side of the tilde (~) is considered the model outcome (here, activity). On the right-hand side of the tilde are the predictors. Variables may be listed by name, or you can use the dot (.) to indicate all other variables as predictors.
2. The data. A recipe is associated with the data set used to create the model. This will typically be the training set, so `data = train_data` here. Naming a data set doesnâ€™t actually change the data itself; it is only used to catalog the names of the variables and their types, like factors, integers, dates, etc.

Now we can add roles to this recipe. We can use the `update_role()` function to let recipes know that `id` is a variable with a custom role that we called "ID" (a role can have any character value). Whereas our formula included all variables in the training set other than activity as predictors (that's what the `.` does), this tells the recipe to keep these two variables but not use them as either outcomes or predictors.

```{r}
activity_recipe <- 
  recipe(activity ~ ., data = train_data) %>% 
  update_role(id, new_role = "ID") %>% 
  update_role(time_s, new_role = "ID") %>%
  step_zv(all_predictors()) ### Remove columns from the data when the training set data have a single value. Zero variance predictor
```

#### Create a workflow

A workflow connects our recipe with out model. The workflow let's us setup the models without actually have run things over and over again. This is helpful because as you will sometimes models can take a long time to run. 

```{r}
activity_workflow <- 
        workflow() %>% 
        add_model(rf_model) %>% 
        add_recipe(activity_recipe)

activity_workflow
```

### Fit a model 

```{r}
set.seed(100)

folds <- vfold_cv(train_data, v = 5) ## normally you would do at least 10 folds. Just doing 5 because it's faster.

activity_fit <- 
      activity_workflow %>% 
      fit_resamples(resamples = folds,
                    control = control_resamples(save_pred = TRUE, 
                                                verbose = FALSE)) ## Edit for running live

activity_fit
```

### Collect the results

```{r}
rf_best <- 
  activity_fit %>% 
  select_best(metric = "roc_auc")

rf_best

rf_auc_fit <- 
  activity_fit %>% 
  collect_predictions(parameters = rf_best) 
```

### Metrics

#### Confusion Matrix

We can generate a confusion matrix by using the `conf_mat()` function by supplying the data frame (`rf_auc_fit`), the truth column `activity` and predicted class `.pred_class` in the estimate attribute.

A confusion matrix is sort of a 2x2 (or n*n table for multiclass problems) table with the true values on one side and predicted values in another column. If we look on the diagonal we see when the model correctly predicts the values `activity` and off diagonal is when the model does not predict the correct value.

Here is the confusion matrix for one fold. 

```{r}
cm <- rf_auc_fit %>%
        dplyr::filter(id == "Fold1") %>%
        conf_mat(activity, .pred_class)
cm
```

* 1. Walking
* 2. Descending stairs
* 3. Ascending stairs
* 4. Driving
* 77. Clapping 
* 99. Non-study activity

If we look at the confusion matrix we can see that the model is overall pretty good. Lots of data on the diagonal. That said, it's also clear the we are not very good at predicting clapping (77) and walking (1) based on the confusion matrix. This tells use that we might need to think about features that could help us explain those classes. For clapping for example, we might do the rolling maximum of x, y, z over a short time window, which might help us pick out those very high values associated with clapping. 

Here is the confusion matrix for all 5 of the folds. 

```{r}
conf_mat(rf_auc_fit, truth = activity,
         estimate = .pred_class)
```

#### Accuracy

We can calculate the classification accuracy by using the `accuracy()` function by supplying the final data frame `rf_auc_fit`, the truth column `activity` and predicted class `.pred_class` in the estimate attribute. 

```{r}
accuracy(rf_auc_fit, truth = activity,
         estimate = .pred_class)
```

#### Sensitivity

```{r}
sens(rf_auc_fit, truth = activity,
         estimate = .pred_class)
```

#### Specificity

```{r}
spec(rf_auc_fit, truth = activity,
         estimate = .pred_class)
```

#### F1 Score

```{r}
f_meas(rf_auc_fit, truth = activity,
         estimate = .pred_class)
```

## Final model

Above we are looking at our trained model over the cross-validation sets. We have not actually tested our model on our test data. To run the last model we need to back to our workflow and use the `last_fit` function. Note that we use the `cv_split` object rather than the train or test data objects. This will will fit the model to the entire training set and evaluate it with the testing set. We need to back to our workflow object (somewhat counter intuitive). 

```{r}
final_rf_model <- last_fit(activity_workflow, cv_split)

collect_metrics(final_rf_model)
```

Overall accuracy on the test data shows this is a pretty good model. Perhaps a bit suspect as I'm always skeptical if I see a model with 0.999 accuracy but this is a very small dataset and sort of a toy example so we are just going to leave it at this. 

```{r}
sessionInfo()
```





